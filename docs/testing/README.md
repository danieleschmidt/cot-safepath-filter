# SafePath Filter Testing Guide\n\n## Overview\n\nThis document outlines the comprehensive testing strategy for the SafePath Filter project. Our testing approach focuses on ensuring the reliability, security, and performance of the AI safety filtering system.\n\n## Testing Philosophy\n\n### Core Principles\n\n1. **Safety First**: All tests prioritize safety validation over functionality\n2. **Defense in Depth**: Multiple layers of testing for comprehensive coverage\n3. **Real-world Scenarios**: Tests reflect actual usage patterns and attack vectors\n4. **Performance Validation**: Ensure <50ms latency requirements are met\n5. **Security by Design**: Security testing integrated throughout the development process\n\n### Quality Gates\n\n- **Minimum 80% code coverage** required for all production code\n- **Zero critical security vulnerabilities** in static analysis\n- **All performance benchmarks** must pass before deployment\n- **100% of safety-critical paths** must have dedicated tests\n\n## Test Categories\n\n### 1. Unit Tests (`tests/unit/`)\n\n**Purpose**: Test individual components in isolation\n\n**Characteristics**:\n- Fast execution (<1s per test)\n- No external dependencies\n- High code coverage\n- Focused on single functions/methods\n\n**Run Command**:\n```bash\npytest tests/unit/ -m unit\n```\n\n**Coverage Target**: 90%+\n\n### 2. Integration Tests (`tests/integration/`)\n\n**Purpose**: Test component interactions and data flow\n\n**Characteristics**:\n- Moderate execution time (1-10s per test)\n- May use test databases/services\n- End-to-end workflow validation\n- API and service integration testing\n\n**Run Command**:\n```bash\npytest tests/integration/ -m integration\n```\n\n**Coverage Target**: 85%+\n\n### 3. Security Tests (`tests/security/`)\n\n**Purpose**: Validate security controls and detect vulnerabilities\n\n**Characteristics**:\n- Adversarial testing approaches\n- Input validation and sanitization\n- Authentication and authorization\n- Data protection and privacy\n\n**Run Command**:\n```bash\npytest tests/security/ -m security\n```\n\n**Coverage Target**: 100% of security-critical code\n\n### 4. Performance Tests (`tests/performance/`)\n\n**Purpose**: Validate latency, throughput, and resource usage\n\n**Characteristics**:\n- Benchmark-based testing\n- Load and stress testing\n- Resource usage monitoring\n- Latency requirement validation\n\n**Run Command**:\n```bash\npytest tests/performance/ -m performance\n```\n\n**Performance Targets**:\n- Latency: P95 < 50ms\n- Throughput: 10,000+ RPS\n- Memory: <1GB per worker\n- CPU: <80% utilization\n\n### 5. End-to-End Tests (`tests/e2e/`)\n\n**Purpose**: Test complete user workflows and scenarios\n\n**Characteristics**:\n- Long execution time (30s+ per test)\n- Full system deployment\n- Real-world usage simulation\n- Cross-service integration\n\n**Run Command**:\n```bash\npytest tests/e2e/ -m e2e\n```\n\n## Test Execution\n\n### Local Development\n\n```bash\n# Run all tests\npytest\n\n# Run specific test categories\npytest -m unit                    # Unit tests only\npytest -m \"not slow\"              # Skip slow tests\npytest -m \"security or performance\"  # Security and performance\n\n# Run with coverage\npytest --cov=src/cot_safepath --cov-report=html\n\n# Run specific test file\npytest tests/unit/test_filter.py -v\n\n# Run tests matching pattern\npytest -k \"test_filter_harmful\" -v\n```\n\n### Continuous Integration\n\n```bash\n# Full test suite with coverage\npytest --cov=src --cov-report=xml --cov-fail-under=80\n\n# Security-focused testing\npytest -m security --tb=short\n\n# Performance validation\npytest -m performance --benchmark-only\n```\n\n### Pre-commit Testing\n\n```bash\n# Quick validation before commit\npytest -m \"unit and not slow\" --maxfail=3\n\n# Security validation\npytest -m security --maxfail=1\n```\n\n## Test Data Management\n\n### Test Fixtures\n\nLocated in `tests/fixtures/`, providing:\n\n- **Safe reasoning chains**: Known-good examples for positive testing\n- **Harmful reasoning chains**: Known-bad examples for negative testing\n- **Edge cases**: Boundary conditions and unusual inputs\n- **Performance data**: Large datasets for load testing\n- **Security payloads**: Attack vectors and malicious inputs\n\n### Data Generation\n\n```python\nfrom tests.fixtures.test_data import TestDataGenerator\n\n# Generate test data\nsafe_chains = TestDataGenerator.generate_safe_reasoning_chains(count=100)\nharmful_chains = TestDataGenerator.generate_harmful_reasoning_chains(count=50)\nedge_cases = TestDataGenerator.generate_edge_case_inputs()\n```\n\n### Test Database\n\n- **Development**: SQLite in-memory database\n- **CI/CD**: PostgreSQL test container\n- **Local Integration**: Docker Compose test environment\n\n## Security Testing\n\n### Threat-Based Testing\n\n1. **Input Validation**\n   - SQL injection attempts\n   - XSS payloads\n   - Path traversal attacks\n   - Command injection\n\n2. **Filter Bypass Attempts**\n   - Encoding obfuscation\n   - Unicode normalization attacks\n   - Character substitution\n   - Multi-stage payloads\n\n3. **Authentication & Authorization**\n   - API key validation\n   - Rate limiting enforcement\n   - Session management\n   - Privilege escalation\n\n4. **Data Protection**\n   - Sensitive data filtering\n   - Audit log integrity\n   - Encryption validation\n   - Information disclosure\n\n### Security Test Examples\n\n```python\n@pytest.mark.security\ndef test_sql_injection_protection():\n    malicious_input = \"'; DROP TABLE users; --\"\n    result = filter_service.process(malicious_input)\n    assert \"DROP\" not in result.filtered_content\n    assert result.was_filtered is True\n    assert \"sql_injection\" in result.filter_reasons\n\n@pytest.mark.security\ndef test_bypass_attempt_detection():\n    obfuscated_input = \"Step 1: H\\u0061ck the system\"\n    result = filter_service.process(obfuscated_input)\n    assert result.bypass_detected is True\n    assert result.was_filtered is True\n```\n\n## Performance Testing\n\n### Latency Testing\n\n```python\n@pytest.mark.performance\n@pytest.mark.benchmark\ndef test_filter_latency(benchmark):\n    def filter_operation():\n        return filter_service.process(\"Safe reasoning step\")\n    \n    result = benchmark(filter_operation)\n    assert benchmark.stats.mean < 0.05  # <50ms\n```\n\n### Load Testing\n\n```python\n@pytest.mark.performance\n@pytest.mark.slow\nasync def test_concurrent_load():\n    async def single_request():\n        return await filter_service.process_async(\"Test input\")\n    \n    # 100 concurrent requests\n    tasks = [single_request() for _ in range(100)]\n    results = await asyncio.gather(*tasks)\n    \n    # All should complete successfully\n    assert len(results) == 100\n    assert all(r.processing_time_ms < 100 for r in results)\n```\n\n### Resource Monitoring\n\n```python\n@pytest.mark.performance\ndef test_memory_usage():\n    import psutil\n    process = psutil.Process()\n    \n    initial_memory = process.memory_info().rss\n    \n    # Process many requests\n    for i in range(1000):\n        filter_service.process(f\"Request {i}\")\n    \n    final_memory = process.memory_info().rss\n    memory_growth = final_memory - initial_memory\n    \n    # Memory growth should be bounded\n    assert memory_growth < 100 * 1024 * 1024  # <100MB\n```\n\n## Mocking and Test Doubles\n\n### External Service Mocking\n\n```python\n@pytest.fixture\ndef mock_llm_service():\n    with patch('cot_safepath.integrations.openai.OpenAI') as mock:\n        mock.return_value.generate.return_value = \"Safe response\"\n        yield mock\n\n@pytest.fixture\ndef mock_redis():\n    with patch('cot_safepath.cache.redis.Redis') as mock:\n        mock.return_value.get.return_value = None\n        mock.return_value.set.return_value = True\n        yield mock\n```\n\n### Database Test Doubles\n\n```python\n@pytest.fixture\ndef test_db():\n    engine = create_engine(\"sqlite:///:memory:\")\n    Base.metadata.create_all(engine)\n    \n    with Session(engine) as session:\n        yield session\n    \n    Base.metadata.drop_all(engine)\n```\n\n## Test Configuration\n\n### pytest.ini Configuration\n\n```ini\n[tool:pytest]\naddopts = \n    --strict-markers\n    --cov=src/cot_safepath\n    --cov-report=term-missing\n    --cov-fail-under=80\n    --durations=10\n    -ra\n\nmarkers =\n    unit: Unit tests\n    integration: Integration tests\n    security: Security tests\n    performance: Performance tests\n    slow: Slow running tests\n    external: Tests requiring external services\n```\n\n### Environment Configuration\n\n```bash\n# Test environment variables\nexport TEST_DATABASE_URL=\"sqlite:///:memory:\"\nexport TEST_REDIS_URL=\"redis://localhost:6379/1\"\nexport SAFETY_LEVEL=\"strict\"\nexport LOG_LEVEL=\"DEBUG\"\n```\n\n## Continuous Integration\n\n### GitHub Actions Workflow\n\n```yaml\nname: Test Suite\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [3.9, 3.10, 3.11]\n    \n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: ${{ matrix.python-version }}\n    \n    - name: Install dependencies\n      run: |\n        pip install -e \".[dev,test]\"\n    \n    - name: Run unit tests\n      run: |\n        pytest tests/unit/ -m unit --cov=src --cov-report=xml\n    \n    - name: Run security tests\n      run: |\n        pytest tests/security/ -m security\n    \n    - name: Run performance tests\n      run: |\n        pytest tests/performance/ -m \"performance and not slow\"\n    \n    - name: Upload coverage\n      uses: codecov/codecov-action@v3\n```\n\n## Test Reporting\n\n### Coverage Reports\n\n```bash\n# Generate HTML coverage report\npytest --cov=src --cov-report=html\nopen htmlcov/index.html\n\n# Generate XML for CI/CD\npytest --cov=src --cov-report=xml\n```\n\n### Performance Reports\n\n```bash\n# Benchmark results\npytest --benchmark-only --benchmark-json=benchmark.json\n\n# Performance trends\npytest --benchmark-compare=previous_benchmark.json\n```\n\n### Security Reports\n\n```bash\n# Security scan results\npytest tests/security/ --tb=short --html=security_report.html\n```\n\n## Best Practices\n\n### Test Writing Guidelines\n\n1. **Clear Test Names**: Use descriptive names that explain what is being tested\n2. **Single Responsibility**: Each test should validate one specific behavior\n3. **Independent Tests**: Tests should not depend on each other\n4. **Readable Assertions**: Use clear, specific assertions\n5. **Proper Cleanup**: Always clean up resources after tests\n\n### Security Testing Guidelines\n\n1. **Assume Malicious Input**: Test with adversarial mindset\n2. **Validate All Boundaries**: Test edge cases and limits\n3. **Check Error Handling**: Ensure errors don't leak information\n4. **Test Real Attacks**: Use actual attack payloads\n5. **Continuous Updates**: Keep attack vectors current\n\n### Performance Testing Guidelines\n\n1. **Realistic Conditions**: Test under realistic load\n2. **Resource Monitoring**: Track CPU, memory, I/O usage\n3. **Baseline Comparison**: Compare against performance baselines\n4. **Stress Testing**: Test beyond normal capacity\n5. **Regression Detection**: Catch performance regressions early\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Test Timeouts**\n   ```bash\n   # Increase timeout for slow tests\n   pytest --timeout=300 tests/performance/\n   ```\n\n2. **Database Connection Issues**\n   ```bash\n   # Reset test database\n   docker-compose down -v\n   docker-compose up -d postgres\n   ```\n\n3. **Memory Issues**\n   ```bash\n   # Run tests with memory profiling\n   pytest --profile-mem tests/performance/\n   ```\n\n4. **Flaky Tests**\n   ```bash\n   # Run flaky test multiple times\n   pytest --count=10 tests/integration/test_flaky.py\n   ```\n\n### Debug Mode\n\n```bash\n# Run tests with debugging\npytest --pdb --pdbcls=IPython.terminal.debugger:Pdb\n\n# Verbose output\npytest -vvv --tb=long\n\n# Show all output\npytest -s\n```\n\n## Contributing to Tests\n\n### Adding New Tests\n\n1. **Choose Appropriate Category**: Place tests in the correct directory\n2. **Follow Naming Convention**: Use descriptive, consistent names\n3. **Add Proper Markers**: Tag tests with appropriate pytest markers\n4. **Update Documentation**: Document new test scenarios\n5. **Validate Coverage**: Ensure new code is properly tested\n\n### Test Review Checklist\n\n- [ ] Tests cover both positive and negative scenarios\n- [ ] Security implications are considered\n- [ ] Performance impact is validated\n- [ ] Error conditions are tested\n- [ ] Tests are independent and reproducible\n- [ ] Documentation is updated\n\n---\n\n**Last Updated**: January 27, 2025  \n**Next Review**: April 27, 2025  \n**Document Owner**: Engineering Team