"""
Advanced Monitoring System - Generation 2 Enhanced Observability.

Comprehensive monitoring, metrics collection, and alerting for the pipeline.
"""

import asyncio
import time
import json
import threading
import statistics
from typing import Dict, List, Optional, Any, Callable, Set, Tuple, Union
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from enum import Enum
from collections import defaultdict, deque
import logging
import queue
import pickle
import weakref
import gc
import psutil
import sys

from .models import FilterResult, ProcessingMetrics, SafetyScore
from .self_healing_core import HealthStatus
from .pipeline_diagnostics import DiagnosticFinding, PerformanceSnapshot
from .security_hardening import SecurityThreat


logger = logging.getLogger(__name__)


class MetricType(Enum):
    """Types of metrics that can be collected."""
    COUNTER = "counter"
    GAUGE = "gauge"
    HISTOGRAM = "histogram"
    SUMMARY = "summary"
    TIMER = "timer"


class AlertSeverity(Enum):
    """Alert severity levels."""
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


class MetricScope(Enum):
    """Scope of metric collection."""
    SYSTEM = "system"
    COMPONENT = "component"
    REQUEST = "request"
    SECURITY = "security"
    PERFORMANCE = "performance"


@dataclass
class Metric:
    """A single metric measurement."""
    name: str
    value: Union[int, float]
    metric_type: MetricType
    scope: MetricScope
    timestamp: datetime = field(default_factory=datetime.utcnow)
    labels: Dict[str, str] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "name": self.name,
            "value": self.value,
            "type": self.metric_type.value,
            "scope": self.scope.value,
            "timestamp": self.timestamp.isoformat(),
            "labels": self.labels,
            "metadata": self.metadata
        }


@dataclass
class Alert:
    """An alert generated by the monitoring system."""
    alert_id: str
    title: str
    description: str
    severity: AlertSeverity
    component: str
    timestamp: datetime = field(default_factory=datetime.utcnow)
    metric_name: Optional[str] = None
    threshold: Optional[float] = None
    current_value: Optional[float] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    resolved: bool = False
    resolved_at: Optional[datetime] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "alert_id": self.alert_id,
            "title": self.title,
            "description": self.description,
            "severity": self.severity.value,
            "component": self.component,
            "timestamp": self.timestamp.isoformat(),
            "metric_name": self.metric_name,
            "threshold": self.threshold,
            "current_value": self.current_value,
            "metadata": self.metadata,
            "resolved": self.resolved,
            "resolved_at": self.resolved_at.isoformat() if self.resolved_at else None
        }


@dataclass
class AlertRule:
    """Configuration for an alert rule."""
    name: str
    metric_name: str
    condition: str  # e.g., "greater_than", "less_than", "equals"
    threshold: float
    severity: AlertSeverity
    component: str = "system"
    description: str = ""
    cooldown_minutes: int = 5
    enabled: bool = True


class MetricCollector:
    """Collects and stores metrics from various sources."""
    
    def __init__(self, max_metrics: int = 100000):
        self.max_metrics = max_metrics
        self.metrics: deque = deque(maxlen=max_metrics)
        self.metric_summaries: Dict[str, Dict] = defaultdict(lambda: {
            "count": 0,
            "sum": 0.0,
            "min": float('inf'),
            "max": float('-inf'),
            "avg": 0.0,
            "last_value": None,
            "last_timestamp": None
        })
        
        # Thread safety
        self._lock = threading.Lock()
        
        # Automatic system metrics collection
        self.system_metrics_enabled = True
        self.system_metrics_thread: Optional[threading.Thread] = None
        self.collection_interval = 30  # seconds
        
        logger.info("Metric collector initialized")
    
    def record_metric(self, name: str, value: Union[int, float], 
                     metric_type: MetricType = MetricType.GAUGE,
                     scope: MetricScope = MetricScope.SYSTEM,
                     labels: Dict[str, str] = None,
                     metadata: Dict[str, Any] = None) -> None:
        """Record a metric measurement."""
        metric = Metric(
            name=name,
            value=value,
            metric_type=metric_type,
            scope=scope,
            labels=labels or {},
            metadata=metadata or {}
        )
        
        with self._lock:
            self.metrics.append(metric)
            
            # Update summary statistics
            summary = self.metric_summaries[name]
            summary["count"] += 1
            summary["sum"] += value
            summary["min"] = min(summary["min"], value)
            summary["max"] = max(summary["max"], value)
            summary["avg"] = summary["sum"] / summary["count"]
            summary["last_value"] = value
            summary["last_timestamp"] = metric.timestamp
    
    def record_counter(self, name: str, increment: int = 1, 
                      labels: Dict[str, str] = None) -> None:
        """Record a counter metric."""
        self.record_metric(name, increment, MetricType.COUNTER, labels=labels)
    
    def record_gauge(self, name: str, value: Union[int, float], 
                    labels: Dict[str, str] = None) -> None:
        """Record a gauge metric."""
        self.record_metric(name, value, MetricType.GAUGE, labels=labels)
    
    def record_timer(self, name: str, duration_ms: float, 
                    labels: Dict[str, str] = None) -> None:
        """Record a timer metric."""
        self.record_metric(name, duration_ms, MetricType.TIMER, labels=labels)
    
    def get_metric_summary(self, name: str) -> Optional[Dict[str, Any]]:
        """Get summary statistics for a metric."""
        with self._lock:
            if name in self.metric_summaries:
                return dict(self.metric_summaries[name])
        return None
    
    def get_recent_metrics(self, name: str, minutes: int = 60) -> List[Metric]:
        """Get recent metrics for a specific name."""
        cutoff_time = datetime.utcnow() - timedelta(minutes=minutes)
        
        with self._lock:
            return [
                metric for metric in self.metrics
                if metric.name == name and metric.timestamp >= cutoff_time
            ]
    
    def start_system_metrics_collection(self) -> None:
        """Start automatic system metrics collection."""
        if self.system_metrics_thread and self.system_metrics_thread.is_alive():
            return
        
        self.system_metrics_enabled = True
        self.system_metrics_thread = threading.Thread(target=self._collect_system_metrics)
        self.system_metrics_thread.daemon = True
        self.system_metrics_thread.start()
        logger.info("System metrics collection started")
    
    def stop_system_metrics_collection(self) -> None:
        """Stop automatic system metrics collection."""
        self.system_metrics_enabled = False
        if self.system_metrics_thread:
            self.system_metrics_thread.join(timeout=5)
        logger.info("System metrics collection stopped")
    
    def _collect_system_metrics(self) -> None:
        """Collect system-level metrics in a background thread."""
        while self.system_metrics_enabled:
            try:
                # CPU metrics
                cpu_percent = psutil.cpu_percent(interval=1)
                self.record_gauge("system.cpu.usage_percent", cpu_percent)
                
                cpu_count = psutil.cpu_count()
                self.record_gauge("system.cpu.count", cpu_count)
                
                # Memory metrics
                memory = psutil.virtual_memory()
                self.record_gauge("system.memory.total_bytes", memory.total)
                self.record_gauge("system.memory.used_bytes", memory.used)
                self.record_gauge("system.memory.available_bytes", memory.available)
                self.record_gauge("system.memory.usage_percent", memory.percent)
                
                # Disk metrics
                disk = psutil.disk_usage('/')
                self.record_gauge("system.disk.total_bytes", disk.total)
                self.record_gauge("system.disk.used_bytes", disk.used)
                self.record_gauge("system.disk.free_bytes", disk.free)
                self.record_gauge("system.disk.usage_percent", (disk.used / disk.total) * 100)
                
                # Network metrics
                network = psutil.net_io_counters()
                self.record_counter("system.network.bytes_sent", network.bytes_sent)
                self.record_counter("system.network.bytes_recv", network.bytes_recv)
                self.record_counter("system.network.packets_sent", network.packets_sent)
                self.record_counter("system.network.packets_recv", network.packets_recv)
                
                # Process metrics
                process = psutil.Process()
                self.record_gauge("process.cpu.usage_percent", process.cpu_percent())
                self.record_gauge("process.memory.rss_bytes", process.memory_info().rss)
                self.record_gauge("process.memory.vms_bytes", process.memory_info().vms)
                self.record_gauge("process.threads.count", process.num_threads())
                self.record_gauge("process.file_descriptors.count", process.num_fds())
                
                # Python-specific metrics
                self.record_gauge("python.gc.objects_count", len(gc.get_objects()))
                
                # Wait for next collection
                time.sleep(self.collection_interval)
                
            except Exception as e:
                logger.error(f"Error collecting system metrics: {e}")
                time.sleep(5)  # Brief pause before retrying


class AlertManager:
    """Manages alerts and alert rules."""
    
    def __init__(self, metric_collector: MetricCollector):
        self.metric_collector = metric_collector
        self.alert_rules: Dict[str, AlertRule] = {}
        self.active_alerts: Dict[str, Alert] = {}
        self.alert_history: List[Alert] = []
        self.max_alert_history = 10000
        
        # Alert processing
        self.alert_queue: queue.Queue = queue.Queue()
        self.alert_processors: List[Callable[[Alert], None]] = []
        
        # Monitoring thread
        self.monitoring_active = False
        self.monitoring_thread: Optional[threading.Thread] = None
        self.check_interval = 30  # seconds
        
        self._lock = threading.Lock()
        
        # Register default alert rules
        self._register_default_alert_rules()
        
        logger.info("Alert manager initialized")
    
    def add_alert_rule(self, rule: AlertRule) -> None:
        """Add an alert rule."""
        with self._lock:
            self.alert_rules[rule.name] = rule
        logger.info(f"Added alert rule: {rule.name}")
    
    def remove_alert_rule(self, rule_name: str) -> bool:
        """Remove an alert rule."""
        with self._lock:
            if rule_name in self.alert_rules:
                del self.alert_rules[rule_name]
                logger.info(f"Removed alert rule: {rule_name}")
                return True
        return False
    
    def add_alert_processor(self, processor: Callable[[Alert], None]) -> None:
        """Add an alert processor function."""
        self.alert_processors.append(processor)
    
    def start_monitoring(self) -> None:
        """Start alert monitoring."""
        if self.monitoring_active:
            return
        
        self.monitoring_active = True
        self.monitoring_thread = threading.Thread(target=self._monitoring_loop)
        self.monitoring_thread.daemon = True
        self.monitoring_thread.start()
        logger.info("Alert monitoring started")
    
    def stop_monitoring(self) -> None:
        """Stop alert monitoring."""
        self.monitoring_active = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=5)
        logger.info("Alert monitoring stopped")
    
    def _monitoring_loop(self) -> None:
        """Main alert monitoring loop."""
        while self.monitoring_active:
            try:
                self._check_alert_rules()
                time.sleep(self.check_interval)
            except Exception as e:
                logger.error(f"Error in alert monitoring: {e}")
                time.sleep(5)
    
    def _check_alert_rules(self) -> None:
        """Check all alert rules against current metrics."""
        current_time = datetime.utcnow()
        
        with self._lock:
            rules_to_check = list(self.alert_rules.values())
        
        for rule in rules_to_check:
            if not rule.enabled:
                continue
            
            try:
                self._evaluate_alert_rule(rule, current_time)
            except Exception as e:
                logger.error(f"Error evaluating alert rule {rule.name}: {e}")
    
    def _evaluate_alert_rule(self, rule: AlertRule, current_time: datetime) -> None:
        """Evaluate a single alert rule."""
        # Get recent metrics for the rule
        recent_metrics = self.metric_collector.get_recent_metrics(rule.metric_name, minutes=5)
        
        if not recent_metrics:
            return
        
        # Use the most recent metric value
        current_value = recent_metrics[-1].value
        
        # Check if alert condition is met
        alert_triggered = False
        
        if rule.condition == "greater_than" and current_value > rule.threshold:
            alert_triggered = True
        elif rule.condition == "less_than" and current_value < rule.threshold:
            alert_triggered = True
        elif rule.condition == "equals" and current_value == rule.threshold:
            alert_triggered = True
        elif rule.condition == "not_equals" and current_value != rule.threshold:
            alert_triggered = True
        
        alert_key = f"{rule.component}:{rule.name}"
        
        if alert_triggered:
            # Check if we're in cooldown period
            if alert_key in self.active_alerts:
                existing_alert = self.active_alerts[alert_key]
                cooldown_end = existing_alert.timestamp + timedelta(minutes=rule.cooldown_minutes)
                if current_time < cooldown_end:
                    return  # Still in cooldown
            
            # Create new alert
            alert = Alert(
                alert_id=f"{alert_key}_{int(current_time.timestamp())}",
                title=f"{rule.name} Alert",
                description=rule.description or f"{rule.metric_name} {rule.condition} {rule.threshold}",
                severity=rule.severity,
                component=rule.component,
                metric_name=rule.metric_name,
                threshold=rule.threshold,
                current_value=current_value,
                metadata={"rule_name": rule.name}
            )
            
            with self._lock:
                self.active_alerts[alert_key] = alert
                self.alert_history.append(alert)
                
                # Trim history
                if len(self.alert_history) > self.max_alert_history:
                    self.alert_history = self.alert_history[-self.max_alert_history//2:]
            
            # Queue alert for processing
            try:
                self.alert_queue.put_nowait(alert)
            except queue.Full:
                logger.warning("Alert queue full, dropping alert")
            
            # Process alert immediately
            self._process_alert(alert)
            
        else:
            # Check if we should resolve an existing alert
            if alert_key in self.active_alerts:
                with self._lock:
                    alert = self.active_alerts[alert_key]
                    if not alert.resolved:
                        alert.resolved = True
                        alert.resolved_at = current_time
                        logger.info(f"Resolved alert: {alert.title}")
    
    def _process_alert(self, alert: Alert) -> None:
        """Process an alert by calling all registered processors."""
        for processor in self.alert_processors:
            try:
                processor(alert)
            except Exception as e:
                logger.error(f"Error in alert processor: {e}")
        
        # Default logging
        level = logging.INFO
        if alert.severity == AlertSeverity.WARNING:
            level = logging.WARNING
        elif alert.severity == AlertSeverity.ERROR:
            level = logging.ERROR
        elif alert.severity == AlertSeverity.CRITICAL:
            level = logging.CRITICAL
        
        logger.log(level, f"ALERT: {alert.title} - {alert.description}")
    
    def _register_default_alert_rules(self) -> None:
        """Register default alert rules for common issues."""
        # System resource alerts
        self.add_alert_rule(AlertRule(
            name="high_cpu_usage",
            metric_name="system.cpu.usage_percent",
            condition="greater_than",
            threshold=90.0,
            severity=AlertSeverity.WARNING,
            component="system",
            description="CPU usage is critically high",
            cooldown_minutes=5
        ))
        
        self.add_alert_rule(AlertRule(
            name="high_memory_usage",
            metric_name="system.memory.usage_percent",
            condition="greater_than",
            threshold=85.0,
            severity=AlertSeverity.WARNING,
            component="system",
            description="Memory usage is high",
            cooldown_minutes=5
        ))
        
        self.add_alert_rule(AlertRule(
            name="low_disk_space",
            metric_name="system.disk.usage_percent",
            condition="greater_than",
            threshold=90.0,
            severity=AlertSeverity.ERROR,
            component="system",
            description="Disk space is critically low",
            cooldown_minutes=10
        ))
        
        # Pipeline performance alerts
        self.add_alert_rule(AlertRule(
            name="high_filter_latency",
            metric_name="pipeline.filter.latency_ms",
            condition="greater_than",
            threshold=5000.0,  # 5 seconds
            severity=AlertSeverity.WARNING,
            component="pipeline",
            description="Filter processing latency is high",
            cooldown_minutes=3
        ))
        
        self.add_alert_rule(AlertRule(
            name="high_error_rate",
            metric_name="pipeline.filter.error_rate",
            condition="greater_than",
            threshold=0.05,  # 5%
            severity=AlertSeverity.ERROR,
            component="pipeline",
            description="Filter error rate is high",
            cooldown_minutes=5
        ))
    
    def get_active_alerts(self) -> List[Alert]:
        """Get all active (unresolved) alerts."""
        with self._lock:
            return [alert for alert in self.active_alerts.values() if not alert.resolved]
    
    def get_alert_summary(self, hours: int = 24) -> Dict[str, Any]:
        """Get alert summary for the specified time period."""
        cutoff_time = datetime.utcnow() - timedelta(hours=hours)
        
        with self._lock:
            recent_alerts = [
                alert for alert in self.alert_history
                if alert.timestamp >= cutoff_time
            ]
        
        alerts_by_severity = defaultdict(int)
        alerts_by_component = defaultdict(int)
        
        for alert in recent_alerts:
            alerts_by_severity[alert.severity.value] += 1
            alerts_by_component[alert.component] += 1
        
        return {
            "time_period_hours": hours,
            "total_alerts": len(recent_alerts),
            "active_alerts": len(self.get_active_alerts()),
            "alerts_by_severity": dict(alerts_by_severity),
            "alerts_by_component": dict(alerts_by_component),
            "alert_rules_count": len(self.alert_rules),
            "recent_critical_alerts": [
                alert.to_dict() for alert in recent_alerts
                if alert.severity == AlertSeverity.CRITICAL
            ][-10:]  # Last 10 critical alerts
        }


class PerformanceTracker:
    """Tracks performance metrics and trends."""
    
    def __init__(self, metric_collector: MetricCollector):
        self.metric_collector = metric_collector
        
        # Performance baselines
        self.baselines: Dict[str, float] = {}
        self.baseline_window_hours = 24
        
        # Trend analysis
        self.trend_analysis_enabled = True
        self.trend_window_minutes = 60
        
        logger.info("Performance tracker initialized")
    
    def track_filter_performance(self, filter_result: FilterResult, 
                                processing_time_ms: float,
                                component: str = "filter") -> None:
        """Track performance metrics for a filter operation."""
        # Record basic metrics
        self.metric_collector.record_timer(
            f"pipeline.{component}.latency_ms",
            processing_time_ms,
            labels={"was_filtered": str(filter_result.was_filtered)}
        )
        
        self.metric_collector.record_counter(
            f"pipeline.{component}.requests_total",
            labels={"was_filtered": str(filter_result.was_filtered)}
        )
        
        # Safety score metrics
        if filter_result.safety_score:
            self.metric_collector.record_gauge(
                f"pipeline.{component}.safety_score",
                filter_result.safety_score.overall_score,
                labels={"is_safe": str(filter_result.safety_score.is_safe)}
            )
            
            self.metric_collector.record_gauge(
                f"pipeline.{component}.safety_confidence",
                filter_result.safety_score.confidence
            )
        
        # Content size metrics
        if hasattr(filter_result, 'filtered_content') and filter_result.filtered_content:
            content_size = len(filter_result.filtered_content)
            self.metric_collector.record_gauge(
                f"pipeline.{component}.content_size_bytes",
                content_size
            )
    
    def track_error(self, component: str, error_type: str, 
                   processing_time_ms: Optional[float] = None) -> None:
        """Track error metrics."""
        self.metric_collector.record_counter(
            f"pipeline.{component}.errors_total",
            labels={"error_type": error_type}
        )
        
        if processing_time_ms is not None:
            self.metric_collector.record_timer(
                f"pipeline.{component}.error_latency_ms",
                processing_time_ms,
                labels={"error_type": error_type}
            )
    
    def track_security_event(self, threat: SecurityThreat) -> None:
        """Track security-related metrics."""
        self.metric_collector.record_counter(
            "security.threats_detected_total",
            labels={
                "attack_type": threat.attack_type.value,
                "threat_level": threat.threat_level.value,
                "source_ip": threat.source_ip or "unknown"
            }
        )
    
    def calculate_error_rate(self, component: str, minutes: int = 60) -> float:
        """Calculate error rate for a component."""
        total_requests = self.metric_collector.get_recent_metrics(
            f"pipeline.{component}.requests_total", minutes
        )
        
        error_requests = self.metric_collector.get_recent_metrics(
            f"pipeline.{component}.errors_total", minutes
        )
        
        if not total_requests:
            return 0.0
        
        total_count = sum(metric.value for metric in total_requests)
        error_count = sum(metric.value for metric in error_requests)
        
        if total_count == 0:
            return 0.0
        
        error_rate = error_count / total_count
        
        # Record error rate as a metric
        self.metric_collector.record_gauge(
            f"pipeline.{component}.error_rate",
            error_rate
        )
        
        return error_rate
    
    def get_performance_summary(self, component: str = "filter") -> Dict[str, Any]:
        """Get performance summary for a component."""
        # Latency metrics
        latency_summary = self.metric_collector.get_metric_summary(
            f"pipeline.{component}.latency_ms"
        )
        
        # Safety score metrics
        safety_score_summary = self.metric_collector.get_metric_summary(
            f"pipeline.{component}.safety_score"
        )
        
        # Error rate
        error_rate = self.calculate_error_rate(component)
        
        # Throughput (requests per minute)
        recent_requests = self.metric_collector.get_recent_metrics(
            f"pipeline.{component}.requests_total", minutes=1
        )
        throughput = sum(metric.value for metric in recent_requests)
        
        return {
            "component": component,
            "latency": latency_summary,
            "safety_score": safety_score_summary,
            "error_rate": error_rate,
            "throughput_per_minute": throughput,
            "timestamp": datetime.utcnow().isoformat()
        }


class AdvancedMonitoringManager:
    """Central manager for advanced monitoring capabilities."""
    
    def __init__(self):
        self.metric_collector = MetricCollector()
        self.alert_manager = AlertManager(self.metric_collector)
        self.performance_tracker = PerformanceTracker(self.metric_collector)
        
        # Dashboard data
        self.dashboard_data: Dict[str, Any] = {}
        self.dashboard_update_interval = 30  # seconds
        self.dashboard_thread: Optional[threading.Thread] = None
        
        # Monitoring state
        self.monitoring_active = False
        
        logger.info("Advanced monitoring manager initialized")
    
    def start_monitoring(self) -> None:
        """Start all monitoring components."""
        if self.monitoring_active:
            return
        
        self.monitoring_active = True
        
        # Start metric collection
        self.metric_collector.start_system_metrics_collection()
        
        # Start alert monitoring
        self.alert_manager.start_monitoring()
        
        # Start dashboard updates
        self._start_dashboard_updates()
        
        logger.info("Advanced monitoring started")
    
    def stop_monitoring(self) -> None:
        """Stop all monitoring components."""
        self.monitoring_active = False
        
        # Stop components
        self.metric_collector.stop_system_metrics_collection()
        self.alert_manager.stop_monitoring()
        
        # Stop dashboard updates
        if self.dashboard_thread:
            self.dashboard_thread.join(timeout=5)
        
        logger.info("Advanced monitoring stopped")
    
    def _start_dashboard_updates(self) -> None:
        """Start dashboard data updates."""
        self.dashboard_thread = threading.Thread(target=self._update_dashboard_loop)
        self.dashboard_thread.daemon = True
        self.dashboard_thread.start()
    
    def _update_dashboard_loop(self) -> None:
        """Update dashboard data periodically."""
        while self.monitoring_active:
            try:
                self._update_dashboard_data()
                time.sleep(self.dashboard_update_interval)
            except Exception as e:
                logger.error(f"Error updating dashboard: {e}")
                time.sleep(5)
    
    def _update_dashboard_data(self) -> None:
        """Update dashboard data with current metrics."""
        current_time = datetime.utcnow()
        
        # System overview
        system_metrics = {
            "cpu_usage": self.metric_collector.get_metric_summary("system.cpu.usage_percent"),
            "memory_usage": self.metric_collector.get_metric_summary("system.memory.usage_percent"),
            "disk_usage": self.metric_collector.get_metric_summary("system.disk.usage_percent")
        }
        
        # Pipeline performance
        pipeline_performance = self.performance_tracker.get_performance_summary("filter")
        
        # Security overview
        security_metrics = {
            "threats_detected": self.metric_collector.get_metric_summary("security.threats_detected_total"),
        }
        
        # Alert summary
        alert_summary = self.alert_manager.get_alert_summary(hours=24)
        
        self.dashboard_data = {
            "timestamp": current_time.isoformat(),
            "system": system_metrics,
            "pipeline": pipeline_performance,
            "security": security_metrics,
            "alerts": alert_summary,
            "uptime_seconds": (current_time - datetime.utcnow()).total_seconds(),
            "monitoring_active": self.monitoring_active
        }
    
    def get_dashboard_data(self) -> Dict[str, Any]:
        """Get current dashboard data."""
        return self.dashboard_data.copy()
    
    def add_custom_alert_rule(self, rule: AlertRule) -> None:
        """Add a custom alert rule."""
        self.alert_manager.add_alert_rule(rule)
    
    def add_alert_handler(self, handler: Callable[[Alert], None]) -> None:
        """Add a custom alert handler."""
        self.alert_manager.add_alert_processor(handler)
    
    def record_custom_metric(self, name: str, value: Union[int, float],
                           metric_type: MetricType = MetricType.GAUGE,
                           labels: Dict[str, str] = None) -> None:
        """Record a custom metric."""
        self.metric_collector.record_metric(name, value, metric_type, labels=labels)
    
    def get_monitoring_health(self) -> Dict[str, Any]:
        """Get health status of monitoring components."""
        return {
            "monitoring_active": self.monitoring_active,
            "metric_collector": {
                "metrics_count": len(self.metric_collector.metrics),
                "system_metrics_enabled": self.metric_collector.system_metrics_enabled
            },
            "alert_manager": {
                "active_alerts": len(self.alert_manager.get_active_alerts()),
                "alert_rules": len(self.alert_manager.alert_rules),
                "monitoring_active": self.alert_manager.monitoring_active
            },
            "dashboard": {
                "last_update": self.dashboard_data.get("timestamp"),
                "update_thread_alive": self.dashboard_thread.is_alive() if self.dashboard_thread else False
            }
        }
    
    def cleanup(self) -> None:
        """Cleanup monitoring resources."""
        self.stop_monitoring()
        
        # Clear data
        self.metric_collector.metrics.clear()
        self.metric_collector.metric_summaries.clear()
        self.alert_manager.active_alerts.clear()
        self.alert_manager.alert_history.clear()
        self.dashboard_data.clear()
        
        logger.info("Advanced monitoring manager cleaned up")