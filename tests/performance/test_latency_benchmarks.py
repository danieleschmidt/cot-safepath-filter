\"\"\"Performance tests for SafePath Filter latency requirements.\"\"\"\n\nimport pytest\nimport asyncio\nimport time\nfrom typing import List, Dict, Any\nfrom unittest.mock import Mock, AsyncMock\n\n\nclass TestLatencyBenchmarks:\n    \"\"\"Test latency requirements (<50ms target).\"\"\"\n\n    @pytest.mark.performance\n    @pytest.mark.benchmark\n    def test_filter_latency_single_request(self, benchmark):\n        \"\"\"Benchmark single filter request latency.\"\"\"\n        def filter_operation():\n            # Mock filter operation\n            return self._mock_filter_operation(\"Safe reasoning step\")\n        \n        result = benchmark(filter_operation)\n        assert result[\"was_filtered\"] is False\n        \n        # Check that benchmark is under 50ms\n        stats = benchmark.stats\n        assert stats.mean < 0.05  # 50ms in seconds\n\n    @pytest.mark.performance\n    @pytest.mark.benchmark\n    def test_filter_latency_batch_requests(self, benchmark):\n        \"\"\"Benchmark batch filter request latency.\"\"\"\n        test_inputs = [\n            \"Step 1: Analyze safely\",\n            \"Step 2: Process securely\", \n            \"Step 3: Respond appropriately\",\n            \"Step 4: Monitor results\",\n            \"Step 5: Log outcomes\"\n        ]\n        \n        def batch_filter_operation():\n            return [self._mock_filter_operation(inp) for inp in test_inputs]\n        \n        results = benchmark(batch_filter_operation)\n        assert len(results) == 5\n        \n        # Per-item latency should still be reasonable\n        stats = benchmark.stats\n        per_item_latency = stats.mean / len(test_inputs)\n        assert per_item_latency < 0.02  # 20ms per item\n\n    @pytest.mark.performance\n    @pytest.mark.slow\n    async def test_concurrent_request_latency(self):\n        \"\"\"Test latency under concurrent load.\"\"\"\n        async def single_request():\n            start_time = time.perf_counter()\n            result = await self._mock_async_filter_operation(\"Test reasoning\")\n            end_time = time.perf_counter()\n            return end_time - start_time, result\n        \n        # Test with 10 concurrent requests\n        tasks = [single_request() for _ in range(10)]\n        results = await asyncio.gather(*tasks)\n        \n        latencies = [r[0] for r in results]\n        responses = [r[1] for r in results]\n        \n        # All requests should complete\n        assert len(responses) == 10\n        \n        # Average latency should still be under target\n        avg_latency = sum(latencies) / len(latencies)\n        assert avg_latency < 0.05  # 50ms average\n        \n        # No request should take longer than 100ms\n        max_latency = max(latencies)\n        assert max_latency < 0.1  # 100ms max\n\n    @pytest.mark.performance\n    @pytest.mark.benchmark\n    def test_ml_model_inference_latency(self, benchmark):\n        \"\"\"Benchmark ML model inference latency.\"\"\"\n        def ml_inference():\n            return self._mock_ml_inference(\"Test input for analysis\")\n        \n        result = benchmark(ml_inference)\n        assert \"safety_score\" in result\n        \n        # ML inference should be fast\n        stats = benchmark.stats\n        assert stats.mean < 0.03  # 30ms for ML inference\n\n    @pytest.mark.performance\n    def test_cache_hit_latency(self):\n        \"\"\"Test latency with cache hits (should be <1ms).\"\"\"\n        test_input = \"Cached reasoning step\"\n        \n        # First request (cache miss)\n        start_time = time.perf_counter()\n        result1 = self._mock_filter_with_cache(test_input, cache_hit=False)\n        cache_miss_time = time.perf_counter() - start_time\n        \n        # Second request (cache hit)\n        start_time = time.perf_counter()\n        result2 = self._mock_filter_with_cache(test_input, cache_hit=True)\n        cache_hit_time = time.perf_counter() - start_time\n        \n        assert result1 == result2\n        assert cache_hit_time < 0.001  # <1ms for cache hit\n        assert cache_hit_time < cache_miss_time / 10  # At least 10x faster\n\n    def _mock_filter_operation(self, input_text: str) -> Dict[str, Any]:\n        \"\"\"Mock filter operation with realistic timing.\"\"\"\n        # Simulate some processing time\n        time.sleep(0.01)  # 10ms base processing\n        \n        return {\n            \"filtered_content\": input_text,\n            \"was_filtered\": False,\n            \"safety_score\": 0.95,\n            \"processing_time_ms\": 10,\n            \"filter_reasons\": []\n        }\n\n    async def _mock_async_filter_operation(self, input_text: str) -> Dict[str, Any]:\n        \"\"\"Mock async filter operation.\"\"\"\n        # Simulate async processing\n        await asyncio.sleep(0.01)  # 10ms async processing\n        \n        return {\n            \"filtered_content\": input_text,\n            \"was_filtered\": False,\n            \"safety_score\": 0.95,\n            \"processing_time_ms\": 10,\n            \"filter_reasons\": []\n        }\n\n    def _mock_ml_inference(self, input_text: str) -> Dict[str, Any]:\n        \"\"\"Mock ML model inference.\"\"\"\n        # Simulate ML processing time\n        time.sleep(0.02)  # 20ms for ML inference\n        \n        return {\n            \"safety_score\": 0.85,\n            \"confidence\": 0.92,\n            \"detected_patterns\": [],\n            \"inference_time_ms\": 20\n        }\n\n    def _mock_filter_with_cache(self, input_text: str, cache_hit: bool) -> Dict[str, Any]:\n        \"\"\"Mock filter operation with caching.\"\"\"\n        if cache_hit:\n            # Cache hit - very fast\n            time.sleep(0.0001)  # 0.1ms for cache retrieval\n        else:\n            # Cache miss - normal processing\n            time.sleep(0.01)  # 10ms for processing\n        \n        return {\n            \"filtered_content\": input_text,\n            \"was_filtered\": False,\n            \"safety_score\": 0.95,\n            \"cached\": cache_hit\n        }\n\n\nclass TestThroughputBenchmarks:\n    \"\"\"Test throughput requirements (target: 10k+ RPS).\"\"\"\n\n    @pytest.mark.performance\n    @pytest.mark.slow\n    async def test_max_throughput(self):\n        \"\"\"Test maximum sustainable throughput.\"\"\"\n        duration_seconds = 10\n        start_time = time.perf_counter()\n        request_count = 0\n        \n        async def process_requests():\n            nonlocal request_count\n            while time.perf_counter() - start_time < duration_seconds:\n                await self._mock_async_filter_operation(f\"Request {request_count}\")\n                request_count += 1\n        \n        # Run multiple workers concurrently\n        workers = [process_requests() for _ in range(5)]\n        await asyncio.gather(*workers)\n        \n        actual_duration = time.perf_counter() - start_time\n        rps = request_count / actual_duration\n        \n        print(f\"Processed {request_count} requests in {actual_duration:.2f}s\")\n        print(f\"Throughput: {rps:.0f} RPS\")\n        \n        # Should achieve reasonable throughput\n        # Note: This is a mock test, real throughput depends on implementation\n        assert rps > 100  # At least 100 RPS in mock test\n\n    @pytest.mark.performance\n    def test_memory_usage_under_load(self):\n        \"\"\"Test memory usage doesn't grow excessively under load.\"\"\"\n        import psutil\n        import os\n        \n        process = psutil.Process(os.getpid())\n        initial_memory = process.memory_info().rss / 1024 / 1024  # MB\n        \n        # Process many requests\n        for i in range(1000):\n            result = self._mock_filter_operation(f\"Request {i}\")\n            # Simulate some memory allocation\n            temp_data = [j for j in range(100)]\n            del temp_data\n        \n        final_memory = process.memory_info().rss / 1024 / 1024  # MB\n        memory_growth = final_memory - initial_memory\n        \n        print(f\"Memory growth: {memory_growth:.2f} MB\")\n        \n        # Memory growth should be reasonable\n        assert memory_growth < 100  # Less than 100MB growth\n\n    def _mock_filter_operation(self, input_text: str) -> Dict[str, Any]:\n        \"\"\"Mock filter operation for throughput testing.\"\"\"\n        # Minimal processing for throughput test\n        return {\n            \"filtered_content\": input_text,\n            \"was_filtered\": False,\n            \"safety_score\": 0.95\n        }\n\n    async def _mock_async_filter_operation(self, input_text: str) -> Dict[str, Any]:\n        \"\"\"Mock async filter operation for throughput testing.\"\"\"\n        # Minimal async processing\n        await asyncio.sleep(0.001)  # 1ms processing\n        return {\n            \"filtered_content\": input_text,\n            \"was_filtered\": False,\n            \"safety_score\": 0.95\n        }\n\n\nclass TestResourceUsageBenchmarks:\n    \"\"\"Test resource usage under various conditions.\"\"\"\n\n    @pytest.mark.performance\n    def test_cpu_usage_efficiency(self):\n        \"\"\"Test CPU usage efficiency during processing.\"\"\"\n        import psutil\n        import os\n        \n        process = psutil.Process(os.getpid())\n        \n        # Monitor CPU usage during processing\n        cpu_percentages = []\n        start_time = time.perf_counter()\n        \n        while time.perf_counter() - start_time < 5:  # Run for 5 seconds\n            # Process some requests\n            for _ in range(10):\n                self._mock_cpu_intensive_operation()\n            \n            cpu_percent = process.cpu_percent()\n            if cpu_percent > 0:  # Only record non-zero readings\n                cpu_percentages.append(cpu_percent)\n            \n            time.sleep(0.1)\n        \n        if cpu_percentages:\n            avg_cpu = sum(cpu_percentages) / len(cpu_percentages)\n            print(f\"Average CPU usage: {avg_cpu:.2f}%\")\n            \n            # CPU usage should be reasonable\n            assert avg_cpu < 80  # Less than 80% CPU usage\n\n    @pytest.mark.performance\n    @pytest.mark.ml\n    def test_model_loading_performance(self):\n        \"\"\"Test ML model loading performance.\"\"\"\n        start_time = time.perf_counter()\n        \n        # Mock model loading\n        model = self._mock_load_ml_model()\n        \n        load_time = time.perf_counter() - start_time\n        \n        assert model is not None\n        assert load_time < 5.0  # Model loading should take less than 5 seconds\n        \n        print(f\"Model loading time: {load_time:.3f} seconds\")\n\n    def _mock_cpu_intensive_operation(self):\n        \"\"\"Mock CPU-intensive operation.\"\"\"\n        # Simulate some CPU work\n        result = sum(i * i for i in range(1000))\n        return result\n\n    def _mock_load_ml_model(self):\n        \"\"\"Mock ML model loading.\"\"\"\n        # Simulate model loading time\n        time.sleep(1.0)  # 1 second loading time\n        return {\"model_type\": \"safety_classifier\", \"loaded\": True}